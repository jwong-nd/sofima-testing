{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Evaluation script, and figuring out shape/type which is not documented in Python. \n",
    "\n",
    "NOTE: aind-registration-evaluation must be pip-installable for production. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_reg import utils\n",
    "\n",
    "import functools as ft\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorstore as ts\n",
    "\n",
    "from ng_link import NgState, link_utils\n",
    "\n",
    "import zarr_io\n",
    "import coarse_registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in images and stuff here\n",
    "READ_BUCKET = 'aind-open-data'\n",
    "WRITE_BUCKET = 'sofima-test-bucket'\n",
    "\n",
    "DATASET = 'diSPIM_645736_2022-12-13_02-21-00/diSPIM.zarr'\n",
    "DOWNSAMPLE_EXP = 2\n",
    "\n",
    "tile_layout = np.array([[0], \n",
    "                        [1]])\n",
    "tile_paths = ['tile_X_0001_Y_0000_Z_0000_CH_0405_cam1.zarr',\n",
    "              'tile_X_0002_Y_0000_Z_0000_CH_0405_cam1.zarr']\n",
    "tile_volumes = []\n",
    "for path in tile_paths: \n",
    "    tile_volumes.append(zarr_io.open_zarr_s3(READ_BUCKET, DATASET + f'/{path}/{DOWNSAMPLE_EXP}').T[:,:,:,0,0])\n",
    "\n",
    "tile_1 = tile_volumes[0]\n",
    "tile_2 = tile_volumes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Id: 0, Bottom Id: 1\n",
      "Top: (0, 0), Bot: (1, 0) [ -1. 325.   0.]\n"
     ]
    }
   ],
   "source": [
    "# Find the offset between these two tiles: \n",
    "cx, cy = coarse_registration.compute_coarse_offsets(tile_layout, tile_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 576, 5966)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0]\n",
      " [ 576  576 5966]]\n",
      "[[  0 325]\n",
      " [576 901]]\n"
     ]
    }
   ],
   "source": [
    "# Running through the script \n",
    "\n",
    "transform = np.array([[1, 0, 0, 0], \n",
    "                      [0, 1, 0, 325], \n",
    "                      [0, 0, 1, -1]])\n",
    "bounds_1, bounds_2 = utils.calculate_bounds(\n",
    "    tile_1.shape, tile_2.shape, transform\n",
    ")\n",
    "\n",
    "print(bounds_1)\n",
    "print(bounds_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sample points in overlapping bounds\n",
    "points = utils.sample_points_in_overlap(\n",
    "    bounds_1=bounds_1,\n",
    "    bounds_2=bounds_2,\n",
    "    numpoints=self.args[\"sampling_info\"][\"numpoints\"],\n",
    "    sample_type=self.args[\"sampling_info\"][\"sampling_type\"],\n",
    "    image_shape=image_1_shape,\n",
    ")\n",
    "\n",
    "# print(\"Points: \", points)\n",
    "\n",
    "# Points that fit in window based on a window size\n",
    "pruned_points = utils.prune_points_to_fit_window(\n",
    "    image_1_shape, points, self.args[\"window_size\"]\n",
    ")\n",
    "\n",
    "discarded_points_window = points.shape[0] - pruned_points.shape[0]\n",
    "LOGGER.info(\n",
    "    f\"\"\"Number of discarded points when prunning\n",
    "    points to window: {discarded_points_window}\"\"\",\n",
    ")\n",
    "\n",
    "# calculate metrics per images\n",
    "metric_per_point = []\n",
    "\n",
    "metric_calculator = ImageMetricsFactory().create(\n",
    "    image_1_data,\n",
    "    image_2_data,\n",
    "    self.args[\"metric\"],\n",
    "    self.args[\"window_size\"],\n",
    ")\n",
    "\n",
    "selected_pruned_points = []\n",
    "\n",
    "for pruned_point in pruned_points:\n",
    "\n",
    "    met = metric_calculator.calculate_metrics(\n",
    "        point=pruned_point, transform=transform\n",
    "    )\n",
    "\n",
    "    if met:\n",
    "        selected_pruned_points.append(pruned_point)\n",
    "        metric_per_point.append(met)\n",
    "\n",
    "# compute statistics\n",
    "metric = self.args[\"metric\"]\n",
    "computed_points = len(metric_per_point)\n",
    "\n",
    "dscrd_pts = points.shape[0] - discarded_points_window - computed_points\n",
    "message = f\"\"\"Computed metric: {metric}\n",
    "\\nMean: {np.mean(metric_per_point)}\n",
    "\\nStd: {np.std(metric_per_point)}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

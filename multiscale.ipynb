{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import dask\n",
    "from distributed import wait\n",
    "import xarray_multiscale\n",
    "from ome_zarr.io import parse_url\n",
    "from ome_zarr.writer import write_multiscale\n",
    "from ome_zarr.format import CurrentFormat\n",
    "import zarr\n",
    "\n",
    "from aind_data_transfer.transformations import ome_zarr\n",
    "from aind_data_transfer.util import chunk_utils\n",
    "\n",
    "import zarr_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install hdf5 plugin locally, then should be no issues. \n",
    "# Push a PR to repo with this change. \n",
    "# -> ujson\n",
    "# -> hdf5plugin\n",
    "# -> kerchunk\n",
    "\n",
    "# Add from utils import ensure_shape_5d, expand_chunks, guess_chunks into transformations/ome_zarr.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyramid_data=[dask.array<array, shape=(1, 1, 14172, 3468, 2304), dtype=uint16, chunksize=(1, 1, 406, 406, 406), chunktype=numpy.ndarray>, dask.array<rechunk-merge, shape=(1, 1, 7086, 1734, 1152), dtype=uint16, chunksize=(1, 1, 406, 406, 406), chunktype=numpy.ndarray>, dask.array<rechunk-merge, shape=(1, 1, 3543, 867, 576), dtype=uint16, chunksize=(1, 1, 306, 306, 306), chunktype=numpy.ndarray>, dask.array<rechunk-merge, shape=(1, 1, 1771, 433, 288), dtype=uint16, chunksize=(1, 1, 510, 433, 288), chunktype=numpy.ndarray>, dask.array<rechunk-merge, shape=(1, 1, 885, 216, 144), dtype=uint16, chunksize=(1, 1, 885, 216, 144), chunktype=numpy.ndarray>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 21:33 Starting write...\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Aware of dask.optimize thing, skipping\n",
    "# NOTE: Aware of dask.config thing, skipping\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s %(message)s\", datefmt=\"%Y-%m-%d %H:%M\")\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "# Primary Input\n",
    "image = zarr_io.open_zarr_gcs('sofima-test-bucket', 'output_level_0_debug.zarr')\n",
    "class SyncAdapter:\n",
    "  \"\"\"Makes it possible to use a TensorStore objects as a numpy array.\"\"\"\n",
    "  \n",
    "  def __init__(self, tstore):\n",
    "    self.tstore = tstore\n",
    "\n",
    "  def __getitem__(self, ind):\n",
    "    return np.array(self.tstore[ind])\n",
    "\n",
    "  def __getattr__(self, attr):\n",
    "    return getattr(self.tstore, attr)\n",
    "\n",
    "  @property\n",
    "  def shape(self):\n",
    "    return self.tstore.shape\n",
    "\n",
    "  @property\n",
    "  def ndim(self):\n",
    "    return self.tstore.ndim\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    return \"uint16\"\n",
    "\n",
    "# Output Paths\n",
    "output_path = 'gs://sofima-test-bucket/'\n",
    "output_name = 'fused.zarr'\n",
    "\n",
    "# Other Input Parameters\n",
    "scale_factor = 2\n",
    "voxel_sizes = (0.176, 0.298, 0.298)\n",
    "\n",
    "# Actual Processing\n",
    "dask_image = dask.array.from_array(SyncAdapter(image))\n",
    "# This is the optimized chunksize\n",
    "chunks = chunk_utils.expand_chunks(chunks=dask_image.chunksize,\n",
    "                                   data_shape=dask_image.shape,\n",
    "                                   target_size=64, # Same as Cameron's code \n",
    "                                   itemsize=dask_image.itemsize) \n",
    "chunks = chunk_utils.ensure_shape_5d(chunks)\n",
    "\n",
    "scale_axis = (1, 1, scale_factor, scale_factor, scale_factor)\n",
    "n_lvls = 5\n",
    "pyramid = xarray_multiscale.multiscale(\n",
    "            dask_image,\n",
    "            xarray_multiscale.reducers.windowed_mean,\n",
    "            scale_axis,  # scale factors\n",
    "            preserve_dtype=True,\n",
    "            chunks=\"auto\",  # can also try \"preserve\", which is the default\n",
    "            )[:n_lvls]\n",
    "\n",
    "pyramid_data = [arr.data for arr in pyramid]\n",
    "print(f'{pyramid_data=}')\n",
    "\n",
    "axes_5d = ome_zarr._get_axes_5d()\n",
    "transforms, chunk_opts = ome_zarr._compute_scales(\n",
    "        len(pyramid),\n",
    "        (scale_factor,) * 3,\n",
    "        voxel_sizes,\n",
    "        chunks, # Can optimize, or simply use dask default chunking. \n",
    "        pyramid[0].shape  # origin optional-- especially for single fused image\n",
    "    )\n",
    "\n",
    "loader = CurrentFormat()\n",
    "store = loader.init_store(output_path, mode='w')\n",
    "\n",
    "root_group = zarr.group(store=store)\n",
    "group = root_group.create_group(output_name, overwrite=True)\n",
    "\n",
    "# Actual Jobs\n",
    "LOGGER.info(\"Starting write...\")\n",
    "t0 = time.time()\n",
    "jobs = write_multiscale(\n",
    "    pyramid,\n",
    "    group=group,\n",
    "    fmt=CurrentFormat(),\n",
    "    axes=axes_5d,\n",
    "    coordinate_transformations=transforms,\n",
    "    storage_options=chunk_opts,\n",
    "    name=None,\n",
    "    compute=False,\n",
    ")\n",
    "if jobs:\n",
    "    LOGGER.info(\"Computing dask arrays...\")\n",
    "    arrs = dask.persist(*jobs)\n",
    "    wait(arrs)\n",
    "write_time = time.time() - t0\n",
    "\n",
    "LOGGER.info(\n",
    "      f\"Finished writing tile {output_name}.\\n\"\n",
    "      f\"Took {write_time}s.\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, can try out implementing custom reader (don't think it is necessary)\n",
    "\n",
    "# Minimal Reader\n",
    "# class ZarrReader(io_utils.DataReader):\n",
    "#   def __init__(self):\n",
    "#     pass\n",
    "\n",
    "#   def as_dask_array(self, chunks: Any = None) -> Array:\n",
    "#     return super().as_dask_array(chunks)\n",
    "  \n",
    "#   def get_shape(self):\n",
    "#     pass\n",
    "\n",
    "#   def get_chunks(self):\n",
    "#     pass\n",
    "\n",
    "#   def get_itemsize(self):\n",
    "#     pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
